# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_XzGh8LcJZeDip8JpfOxHcuLBRGMkFw

**[ Loading and exploring the data](https://)**

We imported the dataset using pandas and displayed the basic information and a few sample rows. This allowed us to understand how many features there are, the types of values in each column, and whether there were missing values or irrelevant data that needed to be cleaned.
"""

import pandas as pd

# Load the dataset
df = pd.read_csv("WA_Fn-UseC_-HR-Employee-Attrition (2).csv")

# Display basic information
df.info()

# Display the first few rows
df.head()

"""# New Section

**Basic data cleaning**

We dropped the following columns:

EmployeeNumber: a unique ID that does not influence attrition.

EmployeeCount: always equals 1, so it provides no variation.

Over18: always equals "Y", so it doesn't help differentiate between employees.

StandardHours: constant for all employees.

We also converted the Attrition column from text ("Yes", "No") to binary (1 for "Yes", 0 for "No"). This step was necessary because machine learning algorithms require numeric target variables.
"""

# Drop columns that are constant or irrelevant
df = df.drop(columns=["EmployeeNumber", "EmployeeCount", "Over18", "StandardHours"])

# Convert 'Attrition' column to binary target (Yes → 1, No → 0)
df["Attrition"] = df["Attrition"].map({"Yes": 1, "No": 0})

"""**Feature selection & Split features and target**

We selected only the features that were most relevant to predicting attrition. This selection was based on domain logic and later confirmed with feature importance analysis. Using fewer, meaningful features helps the model focus on real signals in the data, reduces noise, and simplifies interpretation.
"""

# Keep only the most important features based on feature importance
selected_features = [
    "MonthlyIncome",
    "OverTime",
    "Age",
    "TotalWorkingYears",
    "DailyRate",
    "YearsAtCompany",
    "MonthlyRate",
    "HourlyRate",
    "DistanceFromHome",
    "StockOptionLevel",
    "YearsWithCurrManager",
    "PercentSalaryHike",
    "YearsInCurrentRole",
    "NumCompaniesWorked"
]

X = df[selected_features]
y = df["Attrition"]

"""**Separate numerical and categorical columns**

We separated the features into two lists: one for categorical columns (like OverTime) and one for numerical columns (like Age or MonthlyIncome). This separation was required for applying different preprocessing steps to each type.
"""

# Identify categorical and numerical columns
categorical_cols = X.select_dtypes(include="object").columns.tolist()
numerical_cols = X.select_dtypes(include="number").columns.tolist()

"""**Split the data into training and testing sets**

Splitting the data into training and testing sets
We used train_test_split to divide the data into 80% for training and 20% for testing. This allows us to evaluate how well the model performs on unseen data.
"""

from sklearn.model_selection import train_test_split

# Split the dataset into 80% train and 20% test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""**Handling class imbalance with SMOTE**

Handling class imbalance with SMOTE
The original dataset had many more employees who stayed than those who left. This imbalance causes the model to ignore the minority class (those who left). To fix this, we applied SMOTE (Synthetic Minority Oversampling Technique), which generates synthetic examples of the minority class to balance the training data. This improves the model's ability to detect attrition cases.
"""

!pip install -q imbalanced-learn


from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE



# # Install imbalanced-learn if not already installed
# !pip install -q imbalanced-learn

# # Import SMOTE
# from imblearn.over_sampling import SMOTE

# # Create SMOTE object
# smote = SMOTE(random_state=42)

# # Apply SMOTE only to training data
# X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

# # Optional: print new class distribution
# from collections import Counter
# print("Before SMOTE:", Counter(y_train))
# print("After SMOTE:", Counter(y_train_balanced))

"""**Preprocessing pipelines**

We created two separate preprocessing pipelines: one for numerical features and one for categorical features.

For numerical features, we used a median imputer to fill missing values and a standard scaler to normalize the data.

For categorical features, we used a most-frequent imputer and a OneHotEncoder to convert categories into binary columns.
The two pipelines were combined into a single preprocessing step using ColumnTransformer.
"""

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer

# Pipeline for numeric features
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

# Pipeline for categorical features
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

# Combine preprocessing for both types of features
preprocessor = ColumnTransformer(transformers=[
    ("num", numeric_transformer, numerical_cols),
    ("cat", categorical_transformer, categorical_cols)
])

"""**Choosing the model**

We initially considered using Random Forest, but it was not sufficient. Although Random Forest gave high overall accuracy, it failed to correctly classify many of the employees who left. In other words, the recall for class 1 was very low.
Instead, we chose XGBoost, a more advanced and flexible model that performs well on imbalanced datasets.
We used the scale_pos_weight parameter to give extra importance to class 1 (employees who left). This helped shift the model’s focus toward identifying those at risk of leaving.
"""

# 1) from sklearn.ensemble import RandomForestClassifier

# # Full pipeline: preprocessing + model
# model = ImbPipeline(steps=[
#     ("preprocessor", preprocessor),
#     ("smote", SMOTE(random_state=42)),
#     ("classifier", RandomForestClassifier(random_state=42, class_weight="balanced"))
# ])


# Install XGBoost if needed
!pip install -q xgboost

# Import XGBoost
from xgboost import XGBClassifier

# Count class imbalance for scale_pos_weight
from collections import Counter
counts = Counter(y_train)
scale_weight = counts[0] / counts[1]

# Build pipeline with XGBoost and SMOTE
model = ImbPipeline(steps=[
    ("preprocessor", preprocessor),
    ("smote", SMOTE(random_state=42)),
    ("classifier", XGBClassifier(
        random_state=42,
        scale_pos_weight=scale_weight,
        use_label_encoder=False,
        eval_metric='logloss'
    ))
])

"""**Train the model**

We trained the entire pipeline (preprocessing + SMOTE + XGBoost) using the training data. Because we used a pipeline, all steps were applied consistently, reducing the risk of data leakage.
"""

# Fit the pipeline on training data
model.fit(X_train, y_train)

"""**Feature Importance Visualization**

After training the model, we extracted feature importances from the trained XGBoost classifier. This showed which features had the most influence on the model’s predictions. This step helped validate our feature selection and provided insight into what factors drive attrition.
"""

# Extract the classifier and the preprocessor
classifier = model.named_steps["classifier"]
preprocessor = model.named_steps["preprocessor"]

# Get feature names after preprocessing
feature_names = preprocessor.get_feature_names_out()

# Get feature importances from the trained classifier
importances = classifier.feature_importances_

# Create DataFrame with features and their importances
import pandas as pd
import matplotlib.pyplot as plt

feature_df = pd.DataFrame({
    "Feature": feature_names,
    "Importance": importances
}).sort_values(by="Importance", ascending=False)

# Show top 15 features
top_features = feature_df.head(15)

# Plot
plt.figure(figsize=(10, 6))
plt.barh(top_features["Feature"], top_features["Importance"])
plt.gca().invert_yaxis()
plt.title("Top 15 Most Important Features")
plt.xlabel("Importance")
plt.show()

"""**Model evaluation**

We evaluated the model using classification_report, which provides precision, recall, and f1-score for each class. While the model had good precision for class 0 (employees who stayed), it still missed many of those who left. The recall for class 1 was around 0.26, which was an improvement but still not ideal.
"""

from sklearn.metrics import classification_report

# Make predictions on test data
y_pred = model.predict(X_test)

# Print performance metrics
print(classification_report(y_test, y_pred))

"""model  savin"""

import joblib

# Save the full pipeline including preprocessor + SMOTE + XGBoost model
joblib.dump(model, "attrition_model.pkl")

"""**Custom threshold tuning**

By default, the model considers any probability above 0.5 as a prediction of attrition. We lowered this threshold to 0.3 to make the model more sensitive to potential leavers. This change significantly increased recall for class 1 (to 0.33) while slightly reducing precision and overall accuracy. This tradeoff was intentional, as our goal was to detect as many potential leavers as possible, even at the cost of more false positives.
"""

from sklearn.metrics import classification_report

# Get predicted probabilities (for class 1 - leaving)
y_proba = model.predict_proba(X_test)[:, 1]

# Set custom threshold
threshold = 0.3  # try 0.25 or 0.35 too
y_pred_custom = (y_proba >= threshold).astype(int)

# Print performance with custom threshold
print(f"Evaluation with threshold = {threshold}")
print(classification_report(y_test, y_pred_custom))

"""**Final result**

The final model reached a recall of 0.33 for employees who left, which is more than three times higher than the initial model. Although the overall accuracy dropped slightly, the model became much more useful for the real-world goal of early detection and intervention.
"""

# # ===== הפקת תחזיות סיכון לכל העובדים =====
attrition_probs_all = model.predict_proba(X)[:, 1]
# קראי שוב את הקובץ המקורי כדי שנוכל להוסיף עליו את התחזיות
df_orig = pd.read_csv("WA_Fn-UseC_-HR-Employee-Attrition (2).csv")

# הוספת סיכוי הנטישה לכל עובד
df_orig["AttritionRisk"] = attrition_probs_all

# דירוג רמת סיכון
def assign_risk_level(prob):
    if prob >= 0.6:
        return "High"
    elif prob >= 0.3:
        return "Medium"
    else:
        return "Low"

df_orig["RiskLevel"] = df_orig["AttritionRisk"].apply(assign_risk_level)

# המלצה לפעולה
def get_recommendation(row):
    if row["RiskLevel"] == "High":
        return "⚠️ Immediate action: reduce workload or offer promotion"
    elif row["RiskLevel"] == "Medium":
        return "Monitor employee satisfaction"
    else:
        return "No immediate action required"

df_orig["Recommendation"] = df_orig.apply(get_recommendation, axis=1)

# יצירת שמות רנדומליים
import random
first_names = ["Lior", "Noa", "Avi", "Dana", "Rami", "Shira", "Eli", "Tamar", "Yossi", "Hila"]
last_names = ["Levi", "Cohen", "Mizrahi", "Avraham", "Barak", "Goldman", "Sharabi", "BenDavid", "Peretz", "Azoulay"]

def generate_random_name():
    return random.choice(first_names) + " " + random.choice(last_names)

df_orig["EmployeeName"] = [generate_random_name() for _ in range(len(df_orig))]

# יצירת מזהה + שם לעובד לצורך פילטור נוח
df_orig["EmployeeID_Name"] = (
    "Employee " + (df_orig.index + 1).astype(str).str.zfill(3) + " - " + df_orig["EmployeeName"]
)

# שמירה לקובץ סופי לדשבורד
df_orig.to_excel("dashboard_employees_full.xlsx", index=False)

# הורדה ל-Local
from google.colab import files
files.download("dashboard_employees_full.xlsx")


# df["AttritionRisk"] = attrition_probs_all

# # ===== דירוג רמת סיכון =====
# def assign_risk_level(prob):
#     if prob >= 0.6:
#         return "High"
#     elif prob >= 0.3:
#         return "Medium"
#     else:
#         return "Low"

# df["RiskLevel"] = df["AttritionRisk"].apply(assign_risk_level)

# # ===== המלצה לפעולה לכל עובד =====
# def get_recommendation(row):
#     if row["RiskLevel"] == "High":
#         return "⚠️ Immediate action: reduce workload or offer promotion"
#     elif row["RiskLevel"] == "Medium":
#         return "Monitor employee satisfaction"
#     else:
#         return "No immediate action required"

# df["Recommendation"] = df.apply(get_recommendation, axis=1)

# # ===== שמירת הקובץ לדשבורד בטאבלו =====
# df.to_excel("dashboard_employees.xlsx", index=False)

# # ===== הורדה ל-Local מהמחשב בענן (Colab) =====
# from google.colab import files
# files.download("dashboard_employees.xlsx")







# # ===== הפקת תחזיות סיכון לכל העובדים =====
# attrition_probs_all = model.predict_proba(X)[:, 1]
# df["AttritionRisk"] = attrition_probs_all

# # ===== דירוג רמת סיכון =====
# def assign_risk_level(prob):
#     if prob >= 0.6:
#         return "High"
#     elif prob >= 0.3:
#         return "Medium"
#     else:
#         return "Low"

# df["RiskLevel"] = df["AttritionRisk"].apply(assign_risk_level)

# # ===== המלצה לפעולה לכל עובד =====
# def get_recommendation(row):
#     if row["RiskLevel"] == "High":
#         return "⚠️ Immediate action: reduce workload or offer promotion"
#     elif row["RiskLevel"] == "Medium":
#         return "Monitor employee satisfaction"
#     else:
#         return "No immediate action required"

# df["Recommendation"] = df.apply(get_recommendation, axis=1)

# # ===== יצירת שמות רנדומליים =====
# import random

# first_names = ["Lior", "Noa", "Avi", "Dana", "Rami", "Shira", "Eli", "Tamar", "Yossi", "Hila"]
# last_names = ["Levi", "Cohen", "Mizrahi", "Avraham", "Barak", "Goldman", "Sharabi", "BenDavid", "Peretz", "Azoulay"]

# def generate_random_name():
#     return random.choice(first_names) + " " + random.choice(last_names)

# df["EmployeeName"] = [generate_random_name() for _ in range(len(df))]

# # ===== יצירת מזהה משולב נוח לפילטרים בטאבלו =====
# df["EmployeeID_Name"] = (
#     "Employee " + (df.index + 1).astype(str).str.zfill(3) + " - " + df["EmployeeName"]
# )

# # ===== שמירת הקובץ לדשבורד בטאבלו =====
# df.to_excel("dashboard_employees_named.xlsx", index=False)

# # ===== הורדה ל-Local מהמחשב בענן (Colab) =====
# from google.colab import files
# files.download("dashboard_employees_named.xlsx")